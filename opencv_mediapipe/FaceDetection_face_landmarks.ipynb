{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f46f412-41f2-4b70-a035-a2fff22e05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dddb7-3670-4a50-9a63-661dac9440fe",
   "metadata": {},
   "source": [
    "# Face Detection using MediaPipe\n",
    "\n",
    "## Task Inputs\n",
    "\n",
    "The Face Detector accepts an input of one of the following data types:\n",
    "\n",
    "- Still images  \n",
    "- Decoded video frames  \n",
    "- Live video feed  \n",
    "\n",
    "## Task Outputs\n",
    "\n",
    "The Face Detector outputs the following results:  \n",
    "\n",
    "- Bounding boxes for detected faces in an image frame.  \n",
    "- Coordinates for 6 face landmarks for each detected face.  \n",
    "### Has models\n",
    "1. Blazeface (short range  & long range)\n",
    "2. BlazeFace Sparse\n",
    "\n",
    "### source https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector\n",
    "using short range for now\n",
    "short range works on Single Shot Detector (SSD) convolutional network technique https://arxiv.org/abs/1512.02325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef5cbc2f-e39f-46cd-af9b-3e9595f0a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialisation\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "def detect_faces(frame):\n",
    "    frame_rgb=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results=face_detection.process(frame_rgb)\n",
    "\n",
    "    #drae detections\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            mp_drawing.draw_detection(frame,detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c6b2f9-198c-4845-9735-6aaed8d1ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"error\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print(\"error falied to capture\")\n",
    "\n",
    "    #media pipe cv tasks\n",
    "\n",
    "    #Object detecion\n",
    "\n",
    "    #face_detection\n",
    "    detect_faces(frame)\n",
    "     \n",
    "    cv2.imshow('real-time video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5325abd-966d-42e8-87f6-41a788d1a19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69fce3f-834e-40a9-ae6c-f41a55970e30",
   "metadata": {},
   "source": [
    "# Face landmark detection\n",
    "## Task inputs \t\n",
    "The Face Landmarker accepts an input of one of the following data types:\n",
    "- Still images\n",
    "- Decoded video frames\n",
    "- Live video feed\n",
    "\n",
    "\n",
    "## Task outputs\n",
    "The Face Landmarker outputs the following results:\n",
    "- Bounding boxes for detected faces in an image frame\n",
    "- A complete face mesh for each detected face, with blendshape scores denoting facial expressions and coordinates for facial landmarks.\n",
    "\n",
    "## Model\n",
    "uses a series of model to predict face landmarks\n",
    "- Face detection model: detects the presence of faces with a few key facial landmarks.\n",
    "- Face mesh model: adds a complete mapping of the face. The model outputs an estimate of 478 3-dimensional face landmarks.\n",
    "- Blendshape prediction model: receives output from the face mesh model predicts 52 blendshape scores, which are coefficients representing facial different expressions.\n",
    "\n",
    "## configurations https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker#models\n",
    "\n",
    "## source https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35d20c74-9057-4a1a-abca-3dbbc84db530",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"error\")\n",
    "    exit()\n",
    "#initialize the model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print(\"error falied to capture\")\n",
    "    frame_rgb=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results=face_mesh.process(frame_rgb)\n",
    "\n",
    "    #drae detections\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec\n",
    "            )            \n",
    "        \n",
    "    cv2.imshow('real-time video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103b78f4-caf3-4274-a8da-c1ceef75409f",
   "metadata": {},
   "source": [
    "Trying some more options in Face landmarks detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e6b130-d5e4-4df8-8de0-ed6585196e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Face Landmarker\n",
    "model_path = 'face_landmarker.task'  # Download from MediaPipe's official models\n",
    "base_options = python.BaseOptions(model_asset_path=model_path)\n",
    "options = vision.FaceLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_face_blendshapes=True,\n",
    "    output_facial_transformation_matrixes=True,\n",
    "    num_faces=1)\n",
    "detector = vision.FaceLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c4b456-984b-4a1b-8d48-6e535b406c8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "source for below visualisations https://colab.sandbox.google.com/github/googlesamples/mediapipe/blob/main/examples/face_landmarker/python/%5BMediaPipe_Python_Tasks%5D_Face_Landmarker.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6109732d-baf4-440a-9de7-adb0468420c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw_landmarks \n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    face_landmarks_list = detection_result.face_landmarks\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    for idx in range(len(face_landmarks_list)):\n",
    "        face_landmarks = face_landmarks_list[idx]\n",
    "        face_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        face_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in face_landmarks\n",
    "        ])\n",
    "\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_TESSELATION,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_tesselation_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_CONTOURS,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_contours_style())\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            image=annotated_image,\n",
    "            landmark_list=face_landmarks_proto,\n",
    "            connections=mp.solutions.face_mesh.FACEMESH_IRISES,\n",
    "            landmark_drawing_spec=None,\n",
    "            connection_drawing_spec=mp.solutions.drawing_styles\n",
    "            .get_default_face_mesh_iris_connections_style())\n",
    "\n",
    "    return annotated_image\n",
    "\n",
    "def plot_face_blendshapes_bar_graph(face_blendshapes):\n",
    "    face_blendshapes_names = [face_blendshapes_category.category_name for face_blendshapes_category in face_blendshapes]\n",
    "    face_blendshapes_scores = [face_blendshapes_category.score for face_blendshapes_category in face_blendshapes]\n",
    "    face_blendshapes_ranks = range(len(face_blendshapes_names))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 12))\n",
    "    bar = ax.barh(face_blendshapes_ranks, face_blendshapes_scores)\n",
    "    ax.set_yticks(face_blendshapes_ranks, face_blendshapes_names)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    for score, patch in zip(face_blendshapes_scores, bar.patches):\n",
    "        plt.text(patch.get_x() + patch.get_width(), patch.get_y(), f\"{score:.4f}\", va=\"top\")\n",
    "\n",
    "    ax.set_xlabel('Score')\n",
    "    ax.set_title(\"Face Blendshapes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "187db95b-6493-496e-81cd-adea55643413",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Real-time processing\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert to RGB and process\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=rgb_frame)\n",
    "    detection_result = detector.detect(mp_image)\n",
    "\n",
    "    if detection_result.face_landmarks:\n",
    "        # Draw landmarks\n",
    "        annotated_image = draw_landmarks_on_image(rgb_frame, detection_result)\n",
    "        display_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        # Press 'b' to show blendshapes plot\n",
    "        if cv2.waitKey(1) & 0xFF == ord('b') and detection_result.face_blendshapes:\n",
    "            plot_face_blendshapes_bar_graph(detection_result.face_blendshapes[0])\n",
    "\n",
    "    cv2.imshow('Face Landmarks', display_image)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffc3b86-6efd-489a-b251-8dc7b05500a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8f98ca-cab0-4f6c-9389-56bd182b35d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
