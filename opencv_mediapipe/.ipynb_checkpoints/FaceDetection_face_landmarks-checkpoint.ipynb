{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f46f412-41f2-4b70-a035-a2fff22e05b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44dddb7-3670-4a50-9a63-661dac9440fe",
   "metadata": {},
   "source": [
    "# Face Detection using MediaPipe\n",
    "\n",
    "## Task Inputs\n",
    "\n",
    "The Face Detector accepts an input of one of the following data types:\n",
    "\n",
    "- Still images  \n",
    "- Decoded video frames  \n",
    "- Live video feed  \n",
    "\n",
    "## Task Outputs\n",
    "\n",
    "The Face Detector outputs the following results:  \n",
    "\n",
    "- Bounding boxes for detected faces in an image frame.  \n",
    "- Coordinates for 6 face landmarks for each detected face.  \n",
    "### Has models\n",
    "1. Blazeface (short range  & long range)\n",
    "2. BlazeFace Sparse\n",
    "\n",
    "### source https://ai.google.dev/edge/mediapipe/solutions/vision/face_detector\n",
    "using short range for now\n",
    "short range works on Single Shot Detector (SSD) convolutional network technique https://arxiv.org/abs/1512.02325"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef5cbc2f-e39f-46cd-af9b-3e9595f0a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model initialisation\n",
    "\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "def detect_faces(frame):\n",
    "    frame_rgb=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results=face_detection.process(frame_rgb)\n",
    "\n",
    "    #drae detections\n",
    "    if results.detections:\n",
    "        for detection in results.detections:\n",
    "            mp_drawing.draw_detection(frame,detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c6b2f9-198c-4845-9735-6aaed8d1ee17",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"error\")\n",
    "    exit()\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print(\"error falied to capture\")\n",
    "\n",
    "    #media pipe cv tasks\n",
    "\n",
    "    #Object detecion\n",
    "\n",
    "    #face_detection\n",
    "    detect_faces(frame)\n",
    "     \n",
    "    cv2.imshow('real-time video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5325abd-966d-42e8-87f6-41a788d1a19e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a69fce3f-834e-40a9-ae6c-f41a55970e30",
   "metadata": {},
   "source": [
    "# Face landmark detection\n",
    "## Task inputs \t\n",
    "The Face Landmarker accepts an input of one of the following data types:\n",
    "- Still images\n",
    "- Decoded video frames\n",
    "- Live video feed\n",
    "\n",
    "\n",
    "## Task outputs\n",
    "The Face Landmarker outputs the following results:\n",
    "- Bounding boxes for detected faces in an image frame\n",
    "- A complete face mesh for each detected face, with blendshape scores denoting facial expressions and coordinates for facial landmarks.\n",
    "\n",
    "## Model\n",
    "uses a series of model to predict face landmarks\n",
    "- Face detection model: detects the presence of faces with a few key facial landmarks.\n",
    "- Face mesh model: adds a complete mapping of the face. The model outputs an estimate of 478 3-dimensional face landmarks.\n",
    "- Blendshape prediction model: receives output from the face mesh model predicts 52 blendshape scores, which are coefficients representing facial different expressions.\n",
    "\n",
    "## configurations https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker#models\n",
    "\n",
    "## source https://ai.google.dev/edge/mediapipe/solutions/vision/face_landmarker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e52d75cf-026a-455a-83be-dfd43d0214f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"error\")\n",
    "    exit()\n",
    "#initialize the model\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "face_mesh = mp_face_mesh.FaceMesh(\n",
    "    static_image_mode=False,\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5\n",
    ")\n",
    "\n",
    "\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if not ret:\n",
    "        print(\"error falied to capture\")\n",
    "    frame_rgb=cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    results=face_mesh.process(frame_rgb)\n",
    "\n",
    "    #drae detections\n",
    "    if results.multi_face_landmarks:\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                image=frame,\n",
    "                landmark_list=face_landmarks,\n",
    "                connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=drawing_spec,\n",
    "                connection_drawing_spec=drawing_spec\n",
    "            )            \n",
    "        \n",
    "    cv2.imshow('real-time video', frame)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "      break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2609829c-e747-411e-92eb-f21b06a19e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187db95b-6493-496e-81cd-adea55643413",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
